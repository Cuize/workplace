{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "path=\"../../Downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")\n",
    "allData = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## word embedding start\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embedding(file):\n",
    "    if file == path+'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "def make_embedding_matrix(embedding, tokenizer, len_voc,is_zero = False):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    if is_zero:\n",
    "        embedding_matrix = np.zeros((len_voc, embed_size))\n",
    "    else:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "def make_tokenizer(texts, len_voc):\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = make_tokenizer(allData.question_text.values,15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path+'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "word2vec = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qid', 'question_text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### sentence2vec\n",
    "allData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen2vec1(data):\n",
    "    scores=[]\n",
    "    for sen in data.question_text:\n",
    "        words=sen.split()\n",
    "        tmp=[]\n",
    "        for wd in words:\n",
    "            if wd in word2vec:\n",
    "                tmp.append(word2vec[wd])\n",
    "        tmp=np.array(tmp)\n",
    "        if len(tmp)==0:\n",
    "            scores.append(np.array([0]*300))\n",
    "        else:\n",
    "            scores.append(tmp.mean(axis=0))\n",
    "    data['scores']=scores\n",
    "                \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec1(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split trian to trainT, trainV (for tune) and trainTe 8:1:1\n",
    "X=allData['scores']\n",
    "Y=allData['target']\n",
    "X,Xp=X[:len(train)],X[len(train):]\n",
    "Y=Y[:len(train)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "unif=np.random.uniform(size=len(train))\n",
    "Xtr=[]\n",
    "Ytr=[]\n",
    "Xte=[]\n",
    "Yte=[]\n",
    "Xv=[]\n",
    "Yv=[]\n",
    "for i,num in enumerate(unif):\n",
    "    if num<0.8:\n",
    "        Xtr.append(X[i])\n",
    "        Ytr.append(Y[i])\n",
    "    elif num<0.9:\n",
    "        Xte.append(X[i])\n",
    "        Yte.append(Y[i])\n",
    "    else:\n",
    "        Xv.append(X[i])\n",
    "        Yv.append(Y[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr=np.vstack(Xtr)\n",
    "Ytr=np.array(Ytr)\n",
    "Xte=np.vstack(Xte)\n",
    "Yte=np.array(Yte)\n",
    "Xv=np.vstack(Xv)\n",
    "Yv=np.array(Yv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple GBDT\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.4422         4615.45m\n",
      "         2           0.4271         4268.72m\n",
      "         3           0.4152         3641.16m\n",
      "         4           0.4057         3404.11m\n",
      "         5           0.3977         3178.73m\n",
      "         6           0.3902         2968.72m\n",
      "         7           0.3839         2771.47m\n",
      "         8           0.3777         2724.12m\n",
      "         9           0.3722         2651.46m\n",
      "        10           0.3671         2631.62m\n",
      "        11           0.3628         2658.84m\n",
      "        12           0.3584         3335.35m\n",
      "        13           0.3545         7729.71m\n",
      "        14           0.3508         7804.12m\n",
      "        15           0.3474         7367.20m\n",
      "        16           0.3442         6955.19m\n",
      "        17           0.3411         6579.58m\n",
      "        18           0.3384         6251.91m\n",
      "        19           0.3357         5995.79m\n",
      "        20           0.3330         5745.87m\n",
      "        21           0.3307         5495.93m\n",
      "        22           0.3284         5309.06m\n",
      "        23           0.3263         5090.25m\n",
      "        24           0.3243         4919.85m\n",
      "        25           0.3224         4776.15m\n",
      "        26           0.3205         4711.84m\n",
      "        27           0.3187         4576.08m\n",
      "        28           0.3169         4470.97m\n",
      "        29           0.3154         4363.04m\n",
      "        30           0.3138         4295.89m\n",
      "        31           0.3123         4194.43m\n",
      "        32           0.3109         4098.27m\n",
      "        33           0.3096         4059.55m\n",
      "        34           0.3083         3966.90m\n",
      "        35           0.3071         3891.85m\n",
      "        36           0.3059         3820.16m\n",
      "        37           0.3048         3775.18m\n",
      "        38           0.3037         3724.67m\n",
      "        39           0.3026         3677.47m\n",
      "        40           0.3015         3638.92m\n",
      "        41           0.3005         3606.94m\n",
      "        42           0.2996         3548.07m\n",
      "        43           0.2986         3480.73m\n",
      "        44           0.2977         3422.30m\n",
      "        45           0.2969         3351.24m\n",
      "        46           0.2960         3282.63m\n",
      "        47           0.2952         3211.56m\n",
      "        48           0.2944         3160.01m\n",
      "        49           0.2936         3103.33m\n",
      "        50           0.2929         3045.48m\n",
      "        51           0.2922         2997.41m\n",
      "        52           0.2915         2960.31m\n",
      "        53           0.2907         2920.08m\n",
      "        54           0.2901         2886.77m\n",
      "        55           0.2894         2859.90m\n",
      "        56           0.2888         2836.14m\n",
      "        57           0.2881         2794.31m\n",
      "        58           0.2875         2762.90m\n",
      "        59           0.2868         2735.73m\n",
      "        60           0.2863         2718.86m\n",
      "        61           0.2857         2692.64m\n",
      "        62           0.2851         2660.54m\n",
      "        63           0.2846         2633.19m\n",
      "        64           0.2841         2621.97m\n",
      "        65           0.2836         2581.35m\n",
      "        66           0.2831         2554.00m\n",
      "        67           0.2826         2535.21m\n",
      "        68           0.2821         2528.86m\n",
      "        69           0.2816         2520.51m\n",
      "        70           0.2811         2510.36m\n",
      "        71           0.2807         2533.61m\n",
      "        72           0.2803         2517.12m\n",
      "        73           0.2798         2504.60m\n",
      "        74           0.2793         2477.86m\n",
      "        75           0.2789         2455.77m\n",
      "        76           0.2785         2446.78m\n",
      "        77           0.2780         2424.94m\n",
      "        78           0.2776         2398.41m\n",
      "        79           0.2772         2383.54m\n",
      "        80           0.2768         2363.26m\n",
      "        81           0.2764         2350.76m\n",
      "        82           0.2761         2338.33m\n",
      "        83           0.2757         2325.10m\n",
      "        84           0.2753         2310.67m\n",
      "        85           0.2749         2294.66m\n",
      "        86           0.2746         2282.27m\n",
      "        87           0.2743         2264.57m\n",
      "        88           0.2739         2253.43m\n",
      "        89           0.2736         2230.39m\n",
      "        90           0.2732         2220.38m\n",
      "        91           0.2729         2209.07m\n",
      "        92           0.2726         2199.86m\n",
      "        93           0.2722         2178.85m\n",
      "        94           0.2719         2163.04m\n",
      "        95           0.2716         2148.25m\n",
      "        96           0.2713         2126.39m\n",
      "        97           0.2710         2110.79m\n",
      "        98           0.2707         2097.13m\n",
      "        99           0.2704         2091.46m\n",
      "       100           0.2701         2077.17m\n",
      "       101           0.2698         2166.59m\n",
      "       102           0.2695         2175.80m\n",
      "       103           0.2692         2161.36m\n",
      "       104           0.2689         2144.60m\n",
      "       105           0.2687         2129.82m\n",
      "       106           0.2684         2117.35m\n",
      "       107           0.2681         2101.44m\n",
      "       108           0.2678         2089.98m\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=200,max_depth=5,verbose=2).fit(Xtr,Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(Xv, Yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
